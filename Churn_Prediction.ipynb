{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages.\n",
    "\n",
    "import numpy as np                  # Mathetimatical Operations\n",
    "import pandas as pd                 # Data manipulation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     # Used for plotting graphs.\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "df = pd.read_csv('F:/LBSIM/FRP/Dataset/Churn Prediction in Telecom Industry/430-AmitGupta.csv')\n",
    "df.head() # shows top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------  Data type Count  ----- \\n\",df.dtypes.value_counts())\n",
    "cate = [key for key in dict(df.dtypes) if dict(df.dtypes)[key] in ['bool', 'object']]\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in cate:\n",
    "    le.fit(df[i])\n",
    "    df[i] = le.transform(df[i])\n",
    "\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Univariate analysis\n",
    "print('Total number of observations in the dataset are:',df.shape[0]) # Shape() function returns the dimensions of the array. \n",
    "df.info() # Gives the structure of the data w.r.t. different columns.\n",
    "#pd.set_option('display.expand_frame_repr', False)\n",
    "#df.describe()\n",
    "\n",
    "# Plotting the classes for churn and did not churn\n",
    "count_classes = pd.value_counts(df['churn'], sort = True)  # This gives the different set of values that the column 'apply' can take. Also, it plots the graph to show the counts of rows having both the values.\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Churn Rate\")\n",
    "plt.xticks(range(2))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");\n",
    "\n",
    "# Churn variable analysis\n",
    "df['churn'].value_counts()\n",
    "print('Count of number of customers who didnt churn:',df['churn'].value_counts()[0])\n",
    "print('Count of number of customers who churned:',df['churn'].value_counts()[1])\n",
    "percent_churn = df['churn'].value_counts()[0]/df['churn'].value_counts()[1]\n",
    "percent_churn = \"{0:.2f}\".format(percent_churn)\n",
    "percent_churn\n",
    "print('As per the data, the percentage of people who have churned in proportion to the people who didnt churn is ',percent_churn,'%')\n",
    "\n",
    "df_temp = df.copy() # For plotting the correlation matrix\n",
    "df_temp.info()\n",
    "df_temp.drop([\"state\",\"churn\"], axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the correlation between the features\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "corr = df_temp.corr()\n",
    "hm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"Reds\",fmt='.2f',\n",
    "linewidths=.05)\n",
    "f.subplots_adjust(top=0.93)\n",
    "t= f.suptitle('Variable Correlation Heatmap', fontsize=14)\n",
    "# Insights: Some of the highly correlated variables ares:\n",
    "# 1. total_day_minutes and total_day_charge - 1.0\n",
    "# 2. total_eve_minutes and total_eve_charge - 1.0\n",
    "# 3. total_night_minutes and total_night_charge -1.0\n",
    "# 4. total_intl_minutes and total_intl_charge - 1.0\n",
    "# 5. voice_mail_plan_yes and number_vmail_messages - 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the feature importance\n",
    "X = df.iloc[:,1:20]  #independent columns\n",
    "y = df.iloc[:,-1]    #target column i.e churn \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(19).plot(kind='barh')\n",
    "plt.show()\n",
    "# Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n",
    "# Using feature importance and the correlation matrix, for the correlated variables, we can remove the variable which is less important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing number_vmail_messages, total_day_minutes, total_eve_minutes, total_night_minutes, total_intl_minutes\n",
    "df_original=df.copy()\n",
    "df_original.info()\n",
    "df.drop([\"number_vmail_messages\",\"total_day_charge\",\"total_eve_charge\",\"total_night_charge\",\"total_intl_charge\"], axis = 1, inplace = True) \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value detection\n",
    "def missing_data(df):\n",
    "    \"\"\"df: panda data frame\"\"\"\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = total / len(df) \n",
    "    return pd.concat([total,percent], axis=1, keys =['Total', 'Percent'])\n",
    "\n",
    "missing_data(df) # No missing values in the original data. Only missing or null values for the binned variables created.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis For Numeric Variables\n",
    "# Getting Five number summary for all variables\n",
    "df.iloc[:,1:6].describe()\n",
    "df.iloc[:,6:11].describe()\n",
    "df.iloc[:,11:16].describe()\n",
    "\n",
    "# Univariate Analysis For categorical variable\n",
    "df.state.value_counts() # Getting the counts for records state wise.\n",
    "df.international_plan_yes.value_counts()\n",
    "df.voice_mail_plan_yes.value_counts()\n",
    "sns.distplot(df['total_intl_minutes'], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Feature Engineering\n",
    "df_bivariate =df.copy()\n",
    "df_bivariate.info()\n",
    "\n",
    "# Interval Creation\n",
    "df_bivariate['total_day_calls_binned'] = pd.cut(x=df_bivariate['total_day_calls'], bins=[1,30,45,60,75,90,105,120,135,150,165], \n",
    "            labels=['Upto 30', '30-45','45-60','60-75','75-90','90-105','105-120','120-135','135-150','150-165'])\n",
    "df_bivariate['total_day_calls_binned'].value_counts()\n",
    "\n",
    "df_bivariate['total_eve_calls_binned'] = pd.cut(x=df_bivariate['total_eve_calls'], bins=[1,30,45,60,75,90,105,120,135,150,170], \n",
    "            labels=['Upto 30', '30-45','45-60','60-75','75-90','90-105','105-120','120-135','135-150','150-170'])\n",
    "df_bivariate['total_eve_calls_binned'].value_counts()\n",
    "\n",
    "df_bivariate['total_night_calls_binned'] = pd.cut(x=df_bivariate['total_night_calls'], bins=[1,45,60,75,90,105,120,135,150,165,180], \n",
    "            labels=['Upto 45','45-60','60-75','75-90','90-105','105-120','120-135','135-150','150-165','165-180'])\n",
    "df_bivariate['total_night_calls_binned'].value_counts()\n",
    "\n",
    "df_bivariate['total_day_minutes_binned'] = pd.cut(x=df_bivariate['total_day_minutes'], bins=[0,50,100,150,200,250,300,351], \n",
    "            labels=['Upto 50', '50-100', '100-150','150-200','200-250','250-300','300-350',])\n",
    "df_bivariate['total_day_minutes_binned'].value_counts()\n",
    "\n",
    "df_bivariate['total_night_minutes_binned'] = pd.cut(x=df_bivariate['total_night_minutes'], bins=[0,50,100,150,200,250,300,350,400], \n",
    "            labels=['Upto 50', '50-100', '100-150','150-200','200-250','250-300','300-350','350-400'])\n",
    "df_bivariate['total_night_minutes_binned'].value_counts()\n",
    "\n",
    "df_bivariate['total_eve_minutes_binned'] = pd.cut(x=df_bivariate['total_eve_minutes'], bins=[0,50,100,150,200,250,300,350,400], \n",
    "            labels=['Upto 50', '50-100', '100-150','150-200','200-250','250-300','300-350','350-400'])\n",
    "df_bivariate['total_eve_minutes_binned'].value_counts()\n",
    "\n",
    "df_bivariate['total_intl_minutes_binned'] = pd.cut(x=df_bivariate['total_intl_minutes'], bins=[0,5,10,15,20], \n",
    "            labels=[\"Upto 5\", '5-10', '10-15','15-20'])\n",
    "df_bivariate['total_intl_minutes_binned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis\n",
    "# Cross Tabs with Churn variable\n",
    "pd.crosstab(df_bivariate[\"total_day_calls_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_eve_calls_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_night_calls_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_intl_calls_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_day_minutes_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_eve_minutes_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_night_minutes_binned\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"total_intl_minutes_binned\"],df_bivariate[\"churn\"])\n",
    "\n",
    "pd.crosstab(df_bivariate[\"account_length\"],df_bivariate[\"churn\"])\n",
    "\n",
    "pd.crosstab(df_bivariate[\"area_code\"],df[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"international_plan_yes\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"voice_mail_plan_yes\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"number_vmail_messages\"],df_bivariate[\"churn\"])\n",
    "pd.crosstab(df_bivariate[\"customer_service_calls\"],df_bivariate[\"churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building\n",
    "\n",
    "temp2 = ['churn']\n",
    "#Now seperate the dataset as response variable and feature variabes\n",
    "X = df.drop(temp2, axis = 1)\n",
    "#X = wine.drop('quality', axis = 1)\n",
    "y = df['churn']\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Recursive Feature Elimination (Taking all features and then reducing)\n",
    "from sklearn.feature_selection import RFE  # Recursive Feature Elimination\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "rfe = RFE(logreg, 15)\n",
    "rfe = rfe.fit(X,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing state variable\n",
    "cols=['state']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing account_length variable\n",
    "cols=['account_length']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_day_calls variable\n",
    "cols=['total_day_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_eve_calls variable\n",
    "cols=['total_eve_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_night_calls variable\n",
    "cols=['total_night_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_night_minutes variable\n",
    "cols=['total_night_minutes']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination (Taking top 10 features and then reducing)\n",
    "rfe = RFE(logreg,  )\n",
    "rfe = rfe.fit(X,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "X.columns\n",
    "cols=['international_plan_yes', 'voice_mail_plan_yes','total_intl_minutes','total_intl_calls','customer_service_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing state variable\n",
    "cols=['state']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing account_length variable\n",
    "cols=['account_length']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_day_calls variable\n",
    "cols=['total_day_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_night_minutes variable\n",
    "cols=['total_night_minutes']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_eve_calls variable\n",
    "cols=['total_eve_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())\n",
    "#Removing total_night_calls variable\n",
    "cols=['total_night_calls']\n",
    "X = X.drop(cols, axis = 1)\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test splitting of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state= 42)\n",
    "      \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve for Logistic Regression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Churn Rate Prediction')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "pred_svc = svc.predict(X_test)\n",
    "print(classification_report(y_test, pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "#labels = ['No Apply', 'Apply']\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "#labels = ['No Apply', 'Apply']\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
